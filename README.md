As stated in the About section, this project was created solely for using XGBoost for predicting Titanic survival rates.

In my limited understanding, XGBoost is completely overkill whereas SciKit is better suited for this kind of general ML training and testing.
This project, however, is purely for fun, and for testing out Extreme Gradient Boosting.

I had never heard about Boosted Tree algorithms before messing around with the XGB library, so this is as much a learning experience as it is for personal enjoyment.
Please take my code with a grain of salt!
